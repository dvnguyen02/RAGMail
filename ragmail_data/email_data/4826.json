{
  "id": "4826",
  "Subject": "GPT-4.5 Goes Big, Claude 3.7 Reasons, Alexa+ Goes Agentic,\r\n Generating Text Like an Image",
  "From": "\"The Batch @ DeepLearning.AI\" <thebatch@deeplearning.ai>",
  "Date": "Wed, 5 Mar 2025 15:00:02 -0800",
  "Body": "Continuing our discussion on the Voice Stack, I\u2019d like to explore an area that today\u2019s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.\r\n\r\nView in browser (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrL05m_5PW7Y9pgv6lZ3lTW8h1G-76sx4_jW3N3lcs3m88tCW513fV190fKSmW3sK4VP12pTPXW5h9CFv5dmcNmW4QXjX51pkhdNW2hjJs41cxt2BVf9PLW5Ks0NHMbMXSTC6d4lW3BX6Gm7b42J-W1s5vDb84mrvZW7TRVVX5fHwx6W2qnKgS2c2K_VVH0kPg3fDvHqW2jbLsH7QXk6RW1DdXrg2jyyvTW6g7n6S2kM6YFVVg1PC6RJJ2cW5Q5Zd88GVl40N5pQV2WXJmf4W2df2Z71DspLpW6GBdlf3xZJmWW7PCnmr4TLMJ4N7TbxHtndm15MQnBygfBV4FW5Yl8YM25PKB7W46RdbG1J8CWXW8z36_n26CbBCW8R8vNC61Z355W6Rd62h7_9rsrW6R24P56Lcp26N457DbDsKqv8VB3Xj85p0kKMVYjqsR5Wb4RGN17qv7RW_6dLW3f9Hxs3c45HFVKHDk9488-LtN5kTy7-DrjpwVWHLGq2NNqBhN951L8mrl9T9W26yfNg234zzDW3SXkdf8V_fskf6c1Nrv04 )\r\n\r\nThe Batch top banner - March 5, 2025 (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3ncW3zsnKd44TKq5W4SQvVy2rpmGMV3QWSp5mm7N_VRcrXw681cYbVRbskH6Yd04sVG8pDj5BlwQ4W477ylM8Yw9HLW6JWl5S753tN9W5GBlc86ncLK-W3bQrhf5NlZ9QW231k6q6q7n_GML9LDbj_lXJW27MYQS7NC_7JW7bLccp6by062W8PzJ1940mpPKW7hHbjZ3NV0DVW4Pjr_-8Ns7z0W58VHY74D5CB3W3Rl1NV64S0jfW1FhyPq5fqTcRW1YT64Z52KPN_N5gNSfLl469PdNZl2-04 )\r\n\r\nSubscribe (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3l5QzW6N1vHY6lZ3mCW6ZVK0x1NKPYHW6xyND06JFZ3LW5x1GCq3kSy-KW81gYxl6Hk3KLW3HVBMN4fpP2YW6VFvt_15N64MW4KXNyt2cj0NXN66vrnHZ68gqN3hJjXrgkWtGW5jdcYM5sLZb8W6xmVQ24vfH-DW7PjHKn5-zLfSW6PbDW35xBSfkN1KK-9-TwV2pW5MbvPD3NFbbFW2KP_dh1GK17LVMJTND6jdlhgW4K2s5J2KGQ60W28Pj3F1nChr3W7Gqg6T95Fd1lW7W3WQ09gZNRZW41v9Jh4_7D-df34Tl1l04 )   Submit a tip (mailto:thebatch@deeplearning.ai?subject=RE%3A%20Tips%20and%20News)\r\n\r\nDear friends,\r\n\r\nContinuing our discussion on the Voice Stack (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLj3prCCW8wLKSR6lZ3n7W1_FMFx7VXpZ3W5rrWxc6bJNgYW5pjn2Z7zz2K6V-b7jb464Y4zVMB9S23ZNPfKW98cv_27DF4sqVzpNqQ85DhK3W1CQzL21-BYTLW3sfMq23Cd68pN2hYHnXYvVm3W6_ds8V2c0KXnW7wFpp98B8xTvW3DFGTc4qNpWsW2vWbgn3WfJXfVvQt3p5dVcdTW6xLNFF46-kRlW92nX071V_Lm7W3487gq4-lY4vW2_K8Lf80wTc6VKbyw75Z8FRGVbwvg55h5wvLW5jdJpm4pkHR3W7mBJTj2GJsNFW81YBY35Yp9N1N40cHcLrHdydW2_rbgN59PRz9W4MxpQb71Rnp-Vr03_K3NNHdwf3YqXd404 ) , I\u2019d like to explore an area that today\u2019s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.\r\n\r\nWhen communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm.\r\n\r\nA key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user\u2019s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI\u2019s output, and let the user take a turn. This works reasonably well in quiet environments.\r\n\r\nHowever, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD \u2014 which is usually trained to detect human speech \u2014 tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you\u2019ve stopped talking. This is why today\u2019s speech applications often struggle in noisy environments.\r\n\r\nIntriguingly, last year, Kyutai Labs published Moshi (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLC3prCCW95jsWP6lZ3lLV3sPB84X4-qMW8TYhNY7gR8GWW6kvPml7FxPvgW5p-09b1TtyzYN7Ct2G8z6SmJW56FmDT5Pw3C2W44x3BQ7pT2XwW7x51d_46J0QNW8HtMxR4ZR74_VxTs0q4Qb5Q_W4Kslzx6ywKtBW7Dln466YRpPyW1fcl0x1HQ1HWW1rjZ8-2mmDyqW7pZzrF10yqRyF1F7MGNFvBGW5pCPZg6fjWLMW3N7Qyy6kF8FbW3H9fQM6S11J7W6dDccG2RX2k9W7y9BdX5_l_GgN3Zq-pLbLCpsW693NYL24L_3hW8-V3vb7X1Kp2W7YgPhV7CbG5pW3FtQCn3tJQysW2ptrWL1wzBxJW4rZKYM6-pp9NVRBmNj4jxHlqW3tD1zJ41zkXYf6mx8wz04 ) , a model (GitHub (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3q6VkBNQ46wSlXHN64MJl0vYMGwW3vmmc_3pqc_jW4ZBDWz4hRFnSN58_56HPFSzLW5tLnVJ5L8zqrW5--lgM57tNGkW4nvF0t2_KL4SW8gS7K35n5_SsW65pnPd7K2WqyW6FZ5DX8gBwHZW3CXd2r8vbd2BW4rNVy12cPrWBW8369rv90jZYyW7kpNxv7ggPx-W29w_6w1bPN3BN5MVLG0CtlgdMzDR5PbYJvsW8JLht_2TchrpW6r0gq24hPdtvW1Z72PG15YxGKVs1sQN2N0Wssf1PgPhz04 ) ) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user.\r\n\r\nIf you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we\u2019d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It\u2019s listening all the time, and it\u2019s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an \u201cinner monologue\u201d that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.)\r\n\r\nJust as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of \u201creasoning tokens\u201d before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I\u2019m confident we\u2019ll see many more good voice models released this year.\r\n\r\nIt feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in.\r\n\r\nKeep building!\r\n\r\nAndrew\r\n\r\nA MESSAGE FROM DEEPLEARNING.AI (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrK63prCCW69sMD-6lZ3l1V1J1XK4LFhNRW5lGZm75Hp2ZvW3d00cw499KW8W8KQ8pC5YtPH5W8rR-CR3T9F4yW3hL4zk4pvL-wVCNL7X3GdLM5N189QrhhKwnMW51gg8s3Zk37JW1qT_gG52kCWVW7_F2t57HwtxFW7lZPPt7hb6KqW6FCmDn6bxByFW3JYTVB5qN338W3yXhm53DJxgKN49dkY11NVDbW2NrFvc526GDYVM2m--2PKqMsN2WLMZGgygNXN7qm7Msh7g74f5WnvKn04 )\r\n\r\nPromo banner for: \"Event-Driven Agentic Document Workflows\" (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLj3prCCW8wLKSR6lZ3pTVC-R3Q1_Sm1nW8yG7R69j4CxnW7R5PR41f1zB2N3hrMmGkzcGDW8L8NV13TyP0nW7Cs8Rq6j2GXFW5B7kqy61v-CXW2cXTRl5zjD5VW1xZhwT6J7RnbW1z_z341hpbJvVMrKqq3GX1shVkFnW_3_9LGJN5n_7YqwbdygW3SYXS-73_rM0W3T03Fy54QScKW55P-zW2YbL6DW8HD1g134hNdtW3v7d-X2rTg4zW4svbGN7Cm_F1W2VRnG172cwL0W7DS_8j5hZjS6W25VFTG8xmGyLW2qXPMY24w2vHM6mrD2V4y5vW8_Ysrc8syyd7W9hnbzF8PjVCcW898fTq4wWBZWV99GQ-62CL47d3_xfP04 )\r\n\r\nLearn to build an event-driven AI agent that processes documents and fills out forms using RAG, workflows, and human-in-the-loop feedback. This course, built in partnership with LlamaIndex, walks you through designing, building, and refining automated document workflows. Enroll for free (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLj3prCCW8wLKSR6lZ3pTVC-R3Q1_Sm1nW8yG7R69j4CxnW7R5PR41f1zB2N3hrMmGkzcGDW8L8NV13TyP0nW7Cs8Rq6j2GXFW5B7kqy61v-CXW2cXTRl5zjD5VW1xZhwT6J7RnbW1z_z341hpbJvVMrKqq3GX1shVkFnW_3_9LGJN5n_7YqwbdygW3SYXS-73_rM0W3T03Fy54QScKW55P-zW2YbL6DW8HD1g134hNdtW3v7d-X2rTg4zW4svbGN7Cm_F1W2VRnG172cwL0W7DS_8j5hZjS6W25VFTG8xmGyLW2qXPMY24w2vHM6mrD2V4y5vW8_Ysrc8syyd7W9hnbzF8PjVCcW898fTq4wWBZWV99GQ-62CL47d3_xfP04 )\r\n\r\nNews\r\n\r\nText Generation by Diffusion\r\n\r\nTypical large language models are autoregressive, predicting the next token, one at a time, from left to right. A new model hones all text tokens at once.\r\n\r\nWhat\u2019s new: Inception Labs, a Silicon Valley startup, emerged from stealth mode with Mercury Coder (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3kTW9bFLqp5qV1TFW2Lwnv22lWHydW6LQdgL1hrLj3VnwpCj2y_9Z9VB38nF1n3MR2W17HstN3lFPYjW8Y8SLm8QrTDkW1gDtlY3MhclCW1-F0JY3Spp0ZW6mpSMb4m3fMRW5l3dTb9j_vp-W3RsnCY936cXjW46ZSSx2C-G9SW8PHX_r2vP9jKVwNhly1BlbgSW7N0rzm76sXcPW68bSDt5r8V4GW8K2KJ47T5rgNM61FGvjG0q5W2-tdwm6-H-H3W4qDTmw1mbH9WW4nlBcM24Hk9bdB8dH804 ) , a diffusion model that generates code, in small and mini versions. Registered users can try it out here (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3mWW3BYJM53k52mjW2SNVrR2M19VsW6Fp8z68HDDGfVqz0L_8W5hSPW2Yy98737nQV_W8Ppk9t1w-hh0W2nwx0v5_QtqnW3Y4tdn90l3B9N7G1fmjP9gQ1W74RZJk8fLbDXW6JxVzD9k585fVS8W1F8F1WDzN8dTVfwBPglGW99SwT74Tsd8yF1t2XF0XvVpW4v3PXW7XGmpLW1FTptf20fDYsW6WJrtb9jZRwdW2J8M-H1Tk8pDN5lwf6C882dhN2r8k7QszLr2W65gTnw6Kzlnpf6CqHBq04 ) , and an API (sign up for early access here (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3kTW9bFLqp5qV1TFW2Lwnv22lWHydW6LQdgL1hrLj3VnwpCj2y_9Z9VB38nF1n3MR2W17HstN3lFPYjW8Y8SLm8QrTDkW1gDtlY3MhclCW1-F0JY3Spp0ZW6mpSMb4m3fMRW5l3dTb9j_vp-W3RsnCY936cXjW46ZSSx2C-G9SW8PHX_r2vP9jKVwNhly1BlbgSW7N0rzm76sXcPW68bSDt5r8V4GW8K2KJ47T5rgNM61FGvjG0q5W2-tdwm6-H-H3W4qDTmw1mbH9WW4nlBcM24Hk9bdB8dH804 ) ) and on-premises deployments are in the works. The company has not yet announced availability and pricing.\r\n\r\nHow it works: Like image diffusion models, Mercury Coder improves its output over a number of steps by removing noise.\r\n\r\n- Inception Labs shared little information about the model, leaving details including parameter count, input size and output size, training data, and training methods undisclosed.\r\n- An October 2023 paper (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3mkW4St24N7k05gbW5hVWMv9k61fvW7H-KC95HgChnVyTPSx87tyWjW2ksrl991mVB9W92kPP66Shg93VwW2vT37Z4-yW4n9MCh74d723N5Q4chRc1Q3FW165-nS13rcwzW7FMkcY115FLsW9cXV2V5TXVq7W3qjjBX2kpHJCW6dWnyr6NLdSLW8VFtG_4lww79VFrwQY1Phsn-N3RhWvgxf2DCW7qXqRs4sK06TW1kFrQm3d_cZcW95vF-S8zq1BKW8xXZsN9bhHCmW8m99yj4wLMtDdqS2Bq04 ) co-authored by an Inception Labs co-founder describes training a text diffusion model using score entropy. The model learned to estimate the transition ratio between two tokens; that is, the probability that token y is correct over the probability that the current token x is correct.\r\n- In their most successful experiments, the authors added noise to tokens by progressively masking an ever-greater percentage of tokens at random over several steps.\r\n- At inference, the model started with masked tokens and unmasked them over a number of steps. The estimated transition ratio determined how to change each token at each step.\r\n\r\nResults: Mercury Coder\u2019s major advantage is speed, but it also performs well compared to several competitors.\r\n\r\n- The Small and Mini versions are 3.5 to 18 times faster than comparable small coding models. Running on an Nvidia H100 graphics processing unit, Mercury Coder Small generates 737 tokens per second and Mercury Coder Mini generates 1,109 tokens per second. In comparison, Qwen 2.5 Coder 7B generates 207 tokens per second and GPT 4o-Mini generates 59 tokens per second.\r\n- On coding tasks across six benchmarks, Mercury Coder Small outperforms Gemini 2.0 Flash-Lite, Claude 3.5 Haiku, GPT-4o Mini, and Qwen 2.5 Coder 7B on at least four. Mercury Coder Mini beats those models on at least two. Both versions of Mercury Coder lost to DeepSeek Coder V2 Lite on all six benchmarks.\r\n\r\nBehind the news: Several teams have built diffusion models that generate text, but previous efforts have not been competitive with autoregressive large language models (LLMs). Recently, LLaDA (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3lxW8CwRt690c1B8VHLYnW4N5MsGW1wvs0K7j_jyKW2rsWhL2PRdPNW1qHP7P6KxB98W4M4F07854dl1W6Z2Ks885gFCgW8SD3Dz3Y-G53W5pCHKx2HNvdPN6p-8-9Hb4dbW172Q_H1r2B2VW8cqrbN4M_ZCgW2C6R9N3c_MRNW5P1_Ll20XjtsW1nvfbL77LVbtW2RKjVd4GV5TvN5pypmnf69SCW43ygRL6-R8w3W7Z4rp995yppMW8n-Dpc7KV1LGW3MPzFR8MLyXPW8NFrKs1Cf3XRdThYh604 ) showed comparable performance to Meta\u2019s Llama 2 7B but fell short of Llama 3 8B and other similarly sized modern LLMs.\r\n\r\nWhy it matters: Text diffusion models are already faster than autoregressive models. They offer significant promise to accelerate text generation even further.\r\n\r\nWe\u2019re thinking: Diffusion image generators have delivered good output with as little as four or even one step, generating output tokens significantly faster than autoregressive models. If text diffusion models can benefit from improvements in image generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning.\r\n\r\nOpenAI\u2019s GPT-4.5 Goes Big\r\n\r\nOpenAI launched GPT-4.5, which may be its last non-reasoning model.\r\n\r\nWhat\u2019s new: GPT-4.5 is available (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKK3prCCW7lCdLW6lZ3lfW1St9F82F9K-_W6v32644rNk4YW7ScfgY1dmnjGW72xSDB3dNYD_N30bwKmtBr-6W4mx1pK7KsqVSW5kn_3S6lHmFdW3w2kwV2bb_bNW8FfP3r4ybHnqW3d2nzd7P3zJTV7CyjQ6SZxRkW5zXRLX2-zdL_W12Bf2Q6j9z8nW1n3ZTF7pY5zPW6SmbhM34crjLW3M81Xf6GbG86VS80_z4Xk6yrW1QJpZx1hn1MNW812PKj3J3_rVW5MKjN27mv3bfW3pxfBb4wd7xYW5X_GR35sbQqHW4F4pMR22qbJDW4d6Gnw7lNLQNf1507Bb04 ) as a research preview. Unlike OpenAI\u2019s recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it\u2019s a huge model that was trained using a huge amount of computation. As OpenAI\u2019s biggest model to date, GPT-4.5 is very expensive (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3lFW8DZYhX5zqhJ5W4W4lhl3LNDgLW1gsKc11gPyXSN8pvSHwsDRV4W6bs_Ts5nWjmtW5XHVQ93QpypWW7YYNrF2C6bNkW45l4bl2tGyjCW3dnM2V1fGW2HW5F7zCP1VrsNNW4Jw2HC1xq1xgW6XvCL93zD65vN9kPmbY8RMknW4WMpF11bF2KcW86C4W01nRcsXN40Vq7nKNnG6W543Dzt4VhCZrW9f-YqM1NGkGnW5gwjfK6SLtR-W1rW2Xn1L-BSrW6FCTbb7M_2NKW84QqXT2CC1bWf82FNHn04 )  to run, and the company is evaluating whether to offer it via API in the long term.\r\n\r\n- Input/output: text and images in, text out. Voice and video interactions may be available in future updates.\r\n- Availability/price: Via ChatGPT (currently ChatGPT Pro; soon ChatGPT Plus, Team, Enterprise, and Edu) and various APIs (Chat Completions, Assistants, and Batch). $75/$150 per million input/output tokens.\r\n- Features: Web search, function calling, structured output, streaming, system messages, canvas (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLC3prCCW95jsWP6lZ3nSVX89N18z-wL2W2QKrQj8NX2PyW2HcL_n1zrF-hW8s2cPz4ZZ1l2VpfHmg8rQ00_W5F339n8tzbwjW9096GN6lv3cYW1pNbFW5JylQMVl-SRY4GHrz8W8CMf422tzBbtW7xV93q5NsSkjW1nG1LG1Gl46XW6ZC6-h6fBSYlW543lnj26nNsLW27QFPB3FCj1-W7msryX8b6BVgW7YV48r7_M-Q-W6GhKvx93kDdZW57kjvS17HkhhW5kZVn-43RycwW6FYBMp6SL5ZfW56z-J74PTmQVW5J_9dN2hpQxLW5KzlFs6_-JjYW2Kfw7K5h7SfgW2CkzF833HDzMW1MH9_V1hCDpCW2Z4XBZ97HtbkW8Mlq147P1bXjW9dVRTF81_BNkf75Pvrz04 ) collaborative user interface.\r\n- Undisclosed: Parameter count, input and output size, architecture, training data, training methods.\r\n\r\nHow it works: OpenAI revealed few details (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKK3prCCW7lCdLW6lZ3mlW6mxmDw8_m1rDVXxMd81yxGg5W4pbmdS4yffPGW7Q36Py39PZknW75Dcwd1cRvgbW4FLhb060_FCMW5Z9l0z2wdmwSW4M1BL84mZb38W4tpd6y8K1DMYW81NWzq1l404CW3FGz3g67lYqMW4gpysw1PDmx3W95_Hfq647fKPW13HxDm43fJ9lW4x8jp78jkgcQW2-htmC3cPr9cW2TDm-46y5Z1rW6rm9Vr4pz5GHN3cM5x67bLbqW3JNKrX71dF1LW4V_9636v2gK0W2PwxBD1w7qx2VSz0D_60Z-ywW4fb2SJ7_P_Ksf1Fth1q04 ) about how GPT-4.5 was built. The model is bigger than GPT-4o, and it was pretrained and fine-tuned on more data using more computation \u2014 possibly 10x more, given OpenAI\u2019s comment that \u201cwith every new order of magnitude of compute comes novel capabilities.\u201d\r\n\r\n- The model was trained on a combination of publicly available data and data from partnerships and in-house datasets, including data generated by smaller models. Its knowledge cutoff is October 2023.\r\n- The data was filtered for quality, to eliminate personally identifying information, and to eliminate information that might contribute to proliferation of chemical, biological, radiological, and nuclear threats.\r\n- OpenAI developed unspecified techniques to scale up unsupervised pretraining, supervised fine-tuning, and alignment.\r\n\r\nPerformance: \u201cThis isn\u2019t a reasoning model and won\u2019t crush benchmarks,\u201d OpenAI CEO Sam Altman warned in a tweet (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKK3prCCW7lCdLW6lZ3lPW22t_pK5F5StRVbQTt77y_GZ0W460mnv130FhbW7zcg2b6zdthcW6PB0Mk5N1xBGW5Q_0Hg9l-mW0W6QQ7Gs2c5qGQW2rkSP37020mLW6djzdh8H0g0sW33-bP01SRpLVW41c-3K3bq43MW6yN0mz3zdZLPW5fF1DG1hM1pzW5G8vYT7zSC9JW8KSbg23SHfLWW5MG4yZ46RHf7N8l_7w-YvHh0N7WmwyfgHbMYW4gHGCR1szh7RW1c_91M8GJDvNV2V2F97d2dwXW14LwrG1RDDCRW4nrSZg9cmWlYW7VhtYq6XbGNPf8_8RNv04 ) . The company claims that GPT-4.5 offers improved general knowledge, adheres to prompts with more nuance, delivers greater creativity, and has higher emotional intelligence.\r\n\r\n- GPT-4.5 shows less propensity to hallucinate, or confabulate information, than other OpenAI models. On PersonQA (questions that involve publicly available facts about people), GPT-4.5 achieved 78 percent accuracy compared to GPT-4o (28 percent accuracy) and o1 (55 percent accuracy). Moreover, GPT-4.5 achieved a hallucination rate (lower is better) of 0.19 compared to GPT-4o (0.52) and o1 (0.20).\r\n- Its performance on coding benchmarks is mixed. On SWE-Bench Verified (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrL03prCCW7Y8-PT6lZ3pKW5QC56V6kNNQZN5scHVHHzVGLW6GG3xH4WXc0xW97DYWz7xhcspN4YxKjCJtyPnMSP290p5C9xW6WPtk_2PKnWkW60Db-t4HTJ9gV2Mpjv3lMJ99W1PVPrh6TSzkMN4zMR0nCFg16W3Qy_Qq2yKhmFVTxtf_99R56mN65MK4trPhjjW2LPvt64k5dLlVp-S9J3M9h4nW249F586Whg93VqJgMJ5XDVkHW6Zt0_F80t6-rW1CwXJW128RVMW13sZvm1g9vyyN4d6g84rVLzvW89-3QY8_wnLGW6-svRH1jX83rW5s7qfY8jP3BJV4LG_v8J1gl3f126Bfq04 ) , GPT-4.5 achieved a 38 percent pass rate, higher than GPT-4o (30.7 percent) but well below deep research (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrJx5m_5PW50kH_H6lZ3lVW40qtBq1v0NJtN7JvXjchKrVFM_Sv7YbSNP2W5WwTwG1p6m7BVk7rT97KkrMYVVbPRz4ZKq8zW6pD23Q66XfXdW3y7Tdm4Tb3FSW4zyymm1yPD2cW2s-XGx3bmx59W7Qr7Y64DLkDlW881Mp14bxPbZW4KZYt14Z19fDW1DDqmv4xSy91W97F0975n90fSN4vHjcfCd6ZdW2SC_N_81ZXyJW5_3VGM7kmg65W5gK4Sf6LVfFkW81Q1Wv3T0fp4W3c76JG3FRvJZM715V3qBrj_W98j9Q63BB3CVW2WdgNN5C0hJlW5hTTv25kjDrRW7jlZGV1MKPz5Vydhjf10YCGJW4BQPQW5t_qqyW5x3t8j8pmN_bW1gS8nL4N6CstW7MrNf83GxRbFW1VjR532WWG8zf4TgcVv04 ) (61 percent), an agentic workflow that conducts multi-step research on the internet. On SWE-Lancer Diamond (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3mFW3XZFJc2tHXXnW6-smVh1k8M9MW7c88HR76pnjsW3SMXwf4spKqGW8cT_V_7WJ4GrVMJ6846GlZkDVJth0C7nFbv8VmrZgD8sQp6GW9hnh3p8WXH1vW8TCLwt2TJfQBW13HZ1r4Bvmh3W4H52dK1mdpGWW6Z0SGs42YJTDW75Yz3S8Hl2NWW68qZkH5r3hgRW45lyJm28JC_dV9j6BQ5mMt48W83pb_q3WfvsGW5yrSNj7XGhbZW98rMP46frGlYVyF13l49Sl-MW6MKm6h92BMV6f1nvKQ604 ) , which evaluates full-stack software engineering tasks, GPT-4.5 solved 32.6 percent of tasks, outperforming GPT-4o (23.3 percent) and o3-mini (10.8 percent) but again lagging deep research (around 48 percent).\r\n\r\nBehind the news: GPT-4.5\u2019s release comes as OpenAI nears an announced transition (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrL03prCCW7Y8-PT6lZ3nBW2XbKM846q_brN3ywWrMRXyJTW1XdSjg928FPzW3HwZyq9cHZ2PW7nW2Rm3T7PzJW4PQ9Zl91KBwHW4lL_kc3Rb_7YW7kk5Gc4S5-JsW2-wpKs23NH_XW46zQN779vt1pW5JC9zw3skmwcW8f2DxC6HvCw8W21N-Hp3VMqBzW2z3Hby3GDsPvVGpNzR1cWtp_N8fkhmh1c_kWW5bhj1D1LnT7PW5q9fVB7QThH9W5GQfCB2_ttnzW8GvwXP6kPcsyW4NpV253YMZ8KW5hGfdx4VTS-RW2BZhpx5pbsyTVfFxHP7B8Sl4W6yS8-G4mX_dVVVxVQ76nNYs3f3byZpT04 ) away from developing separate general-knowledge and reasoning models. The launch also comes as OpenAI faces an ongoing shortage of processing power. CEO Sam Altman said (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLC3prCCW95jsWP6lZ3nmW3f7mrL4zyHjLW8NrvDd1Gx9jTW47G4ZX71X9MgW6VZJ3Z1F5gMlN2PpJ9v9_vzjN1RmZDWsM_7JW5C8s7X19j8pLW8MjCMP3g85vLVrZ4TN69B5zGW7TBPY_3sttkyW5cYtwv4QkCsnW4yvz45142tDHW8Z615l3tS0SkVB3j7R5kQ1bzW3GpCXD6_cMdyW5fNxHS5yQXKQW41CHFT1YFY_FW6tn46w80S17NW8r-7y48_YvYNW8V4r9x57g4V9W3B9bYz6qYY3_W2NtnNS47KpFrW4kDHRR29Q8XyW52BVVr4xfQxWW4VmV0_491Gp6W3pHHHd7gLXZFVlpJfN5cl-XTV4yRKZ44CX9pW28NBXY9kk3_fW8TSf6K2H62qwf3dnxhK04 )  that the company is \u201cout of GPUs\u201d and struggling to meet demand \u2014 a constraint that may impact whether OpenAI continues to offer GPT-4.5 via API.\r\n\r\nWhy it matters: GPT-4.5 highlights a growing divide in AI research over whether to pursue performance gains by scaling up processing during pretraining or inference. Despite the success of approaches that consume extra processing power at inference, such as agentic techniques and reasoning models such as its own o family, OpenAI clearly still sees value in pretraining larger and larger models.\r\n\r\nWe\u2019re thinking: There\u2019s still more juice to be squeezed out of bigger models! We\u2019re excited to see what the combination of additional compute applied to both pretraining and inference can achieve.\r\n\r\nLearn More About AI With Data Points!\r\n\r\nAI is moving faster than ever. Data Points helps you make sense of it just as fast. Data Points arrives in your inbox twice a week with six brief news stories. This week, we covered OpenAI\u2019s scaled-up Project Orion and the models we\u2019ve been waiting for (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLj3prCCW8wLKSR6lZ3pxW80NNJJ4Ckb4QVBGxDS5hWW3-W8zjWHk4gKwp6W7pVHHk2wxM70W1V-Z2N7X-vHTW1ZTF_B3BrJyBW2dK4ly4ZG8NSW7Q8-Qq5sQrK0N5LfbkgrGH72W1_nQrr8gCJY5W3YWFN95tn871W1FR7gK1t11hvW909mN32F-5qJW8K38Lb7shCbHW1mHmcp1WmG2LW11VS0r2kt60fW5MX6lQ46w6V6W2S_lXB19t7TpW4qFbgK3gdqRqN1g7SglKCHQlN8wKN6RdY4jBW4T31H-2QN-63W1rm9R07zJPDRW1HHD7d6TSyd2V9tTWh5jKl4KVVZlVS6sWDv1W1WHldY3WQXD7TmBF53Rlx95f3c5gWC04 ) . Subscribe today (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrK69bqx4W69v02T6lZ3mzW65tX5X11XyM6W4ksl4B2J9VrQW2BvTcw8nJd22W2K8QXp3gKQH-W7Nx_Sq33vK2fW2RTcDL8fmRSgW8bj-w35n7jCcN4V_b5G-rFjGW76ZWpD7hsz8cW6128WB3vpKX3VvPPHQ403S9nW6Mr3Ys1z1qk5VSml5-1Ys5fgVtjKzk7sbFvnW1Q4Pj04pdH9SVm7ZSv7ptpGsW9jMYfG91D1BjW73y48r8CsT2hW2_PfXN3k3lTBW4tMGyT3XSS3VW4_Tntp4qHhp3W79vmNZ8NrNf_V3bTfn5XrrZHW6z139R213Hy1W1qHJCT7VZbrMW9gdLfr7QB0zyW2tg2qB114MQsW5r6mM_2-4RwyW1QS37r533W4NW8ydfrG5-7GH4W1H_6_l7YdY11Vvxvtd7nTzMPW86XldR3Y-XtFW8S5b568BYwDlN7p7zRVDqNBdW2VVW5d29tkprW1gj9G63534hbW4TdWvp3tXZrSW7ZS7tJ1y8jLQW1_-52W3p7j5hW6ybWmk2wxx_TW3t_-CP4m-pXFW378SnS557YN5N1xTvlmwLDS4W6t2q7C1wLjL_W8JM-2N5gGTQmN1YHSgS1X5y-W4CJJWw9hvPqTW8jcmJ46KJ-S2W6D0G-15CmvH7Vy-7w65KMcnfW4NSm0f2pPZNQW2fzZm68GmQ0JW5qY8wm4MTNbVW2BCcNz4BYPlXW1yFjXm19jxMgW8FT2Y81jD3XxW1MW88j8l9m4jW6JbTrf4Kf1c1Vpn8tq8z36G7W4PCLqY3Q02ZLW1kX9zw67PSRMN8wX2gcfjjYHN3fF1F98NL36N7R20SbBCcykW5408Yh1VT2QJN5hgrY3TJK-tVJlsn38msDrzf3HYhKj04 ) !\r\n\r\nBudget for Reasoning to the Token\r\n\r\nAnthropic\u2019s Claude 3.7 Sonnet implements a hybrid reasoning approach that lets users decide how much thinking they want the model to do before it renders a response.\r\n\r\nWhat\u2019s new: Claude 3.7 Sonnet (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKK3prCCW7lCdLW6lZ3pnW23w8Cq7qHg3LW7n7w3Y9d2TLSW3cfL2K2_KfHVN8D2Lj26WFpjVvS9Cy114bP5W8b7MVb2nQ1cyW1MYf3H65VVQVW4g37MS46J1T0W57T2mn287tmCW3t-blV5VwBswW8pvF1V5Xvns6N6RQ3zcYwmtYW3PFrbJ4Wq2cvW94bwTt6pJ0zPW8qCt3t3RrZ3YW7ylS8X3jsgm5W73hV0Z5dyDrrW6yq2dg5B0tjbW7VZjVb8gzxzfW6vLRwP5wzh8wW3Ny0FS3FBnrkW4S32Q621rspkW5DyC-k87jGCrW2mXQcm3KWxG8f1YPwrP04 ) was trained for strong performance in coding and front-end web development, with less emphasis on math and computer-science competition problems. It implements tool use and computer use (but not web search) and lets users toggle between immediate responses and extended thinking mode (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrL03prCCW7Y8-PT6lZ3krVB6N1038hdyKW7v2MXw27hqP1W7LbYxL3N-64NN4t-yrR9c_c2W8_GwB88Xqc3fW5p08SH3Q388fW1Z_rP013xQlJW6dJ7D01C84jHVXHfx03Fl6Z7W94MC-V46WPsdW1D4pdJ4NLTlWW19b2JQ3Hjr09W7ssmDT6-6wk4W7mFJxx3w_T-XW7WY6M16N9gH1W2C62VK384K4yW3h0jZx15wl4RW17xsgq5nnfCqW5GtWH21WH4JQW3rGG1m48FZvxW27hDzF4DCpCTW3PdRFX9051c8W2yQBlM8CT8TNW7fhj5g6wKGk9W8C2Rz63hQDJTN5tDCrn9qkkSf5r6mHx04 ) , which can improve outputs by allocating a specific number of tokens to reasoning at inference. Like DeepSeek-R1 and Google Gemini Flash Thinking \u2014 and unlike OpenAI o1 \u2014 Claude 3.7 Sonnet fully displays reasoning tokens. Anthropic considers this functionality experimental, so it may change.\r\n\r\n- Input/output: text and images in (up to 200,000 tokens), text out (up to 128,000 tokens).\r\n- Availability/price: Via Anthropic tiers Free (extended thinking not available), Pro, Team, and Enterprise; Anthropic API; Amazon Bedrock; Google Cloud Vertex AI. $3/$15/$15 per million input/output/thinking tokens.\r\n- Undisclosed: parameter count, architecture, training data, training method.\r\n- Anthropic also introduced Claude Code, a command-line tool for AI-assisted coding, which is available as a limited research preview. Claude Code can edit files, write and run tests, commit and push code to GitHub, and use command-line tools.\r\n\r\nHow it works: Anthropic pretrained Claude 3.7 Sonnet on a mix of public and proprietary data (which explicitly did not include Claude users\u2019 inputs and outputs). The team fine-tuned Claude 3.7 Sonnet using constitutional AI (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrL03prCCW7Y8-PT6lZ3l1W4JK4YL2j3byRW8sLgPP8_8QsCW45L-dz35zP6CW2L0RqS378M4fW5jsj0g7W_8nZW7K3klS5DKry3W6CxjqL3srFqQW2sSlRz9clY4wW2n800-5yZbM_N9gZgWdcKth6W5-d9Ln441CsdW8YR72f8yK7HvW3Hg1WJ4nP4mzW1wf80L5VCB36N5wxc_ZPvxCDVNbtNt7MbjsbW78dcM534r3BYW7BNXgf4mCGfBW5YPt_b34GxblW2tcXC_6pMGDlW2ZVFrb8xrT4hW34CC_R1DtygjW3wpPkL3Lt7KcW6JT5hn26MhGbW3gssgN84ZPjFN83FMHSB_QXQdcSw7l04 ) , which encourages a model to follow a set of human-crafted rules.\r\n\r\n- When the model\u2019s extended thinking mode is enabled, API users can control the thinking budget by specifying a number of tokens up to 128,000. (The specified budget is a rough target, so the number of tokens consumed may differ.)\r\n- Anthropic says that extended thinking mode often is more effective given a general instruction to \u201cthink deeply\u201d rather than step-by-step instructions.\r\n- Visible thinking tokens are considered a research preview while Anthropic examines how they affect user interactions with the model. The company highlights three issues: Visible thinking tokens don\u2019t reflect the model\u2019s internal instructions that establish its character and therefore seem to be devoid of personality, they may not reflect the model\u2019s actual reasoning process, and they can reveal flaws that malicious actors may exploit.\r\n- Extended thinking mode processes tokens serially, but Anthropic is experimenting with parallel thinking that follows multiple independent thought processes and chooses the best one according to a majority vote.\r\n\r\nPerformance: Claude 3.7 Sonnet shows exceptional performance in general knowledge, software engineering, and agentic tasks.\r\n\r\n- On the GPQA Diamond (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3lxW6VJNDN83nWCcW3jdkWP7xKLvFN1lf37_bStpNW7Clb_w1kT96vW494Trw7dynWvN3pK7FzTjN6PW1YQbBT7gNfPvW4pWtfZ5HmQCjW4MgBKd7BhBfwW79xh-S8HwgHdW5kqXKG1HlyhdW7dMpGv7BTPqFW4kQKLb21W45gVTHV8319NntVW4T8sN35RQNS4W5LSxQm4T9GDPW95NS6q5FGl-1VbqvL02hQLj1W2v5Kb472vWLTW8dtLs944GMhTW44TBZ94lTq22W2GWkp07KwJ8pf6Jzl2R04 )  (graduate-level science questions), Claude 3.7 Sonnet achieved 84.8 percent in parallel extended thinking mode with a 64,000-token budget. By comparison, X\u2019s Grok 3 beta achieved 84.6 percent (majority voting with 64 tries), and OpenAI\u2019s o3-mini achieved 79.7 percent with high effort.\r\n- On SWE-Bench Verified (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3l7W4RgKbQ1DkGPVW60kdZK35BL-WW7yhRtH7j4Xk4W4BSRNJ4gBm-9W3fwTth434DnJW2ZWv5t2nWHp_W5kLpym3mmX--W8y-W_s8pKGtsW6sF0QF1MkF7CN3b-dX1fPN--N3sz5v4NSWvhW1tYFLC4WFZWpVMW0hV4F7nKnW4dJDx-2Fjky6W3q7zZN2vDYgnN1z4syNbVxwzW5hrV-62v1GJpW64zhQY1yQY7qW4DMvTw39wNryW5qJn_j2F-4CxN4nNFLfqz5B2W1MNDJs7xYKV0f1kFvQK04 ) , which evaluates the ability to solve real-world software engineering problems, Claude 3.7 Sonnet achieved 70.3 percent without extended thinking, averaged over 16 trials. OpenAI\u2019s o3-mini achieved 49.3 percent with high effort, and DeepSeek R1 achieved 49.2 percent with extended thinking, 32,000 tokens.\r\n- TAU-bench evaluates agentic reasoning. On the Retail subset, which assesses performance in product recommendations and customer service, Claude 3.7 Sonnet achieved 81.2 percent without extended thinking, outperforming OpenAI\u2019s o1 (73.5 percent). In the Airline subset, which measures multi-step reasoning in tasks like flight bookings and customer support, Claude 3.7 Sonnet achieved 58.4 percent, likewise ahead of o1 (54.2 percent).\r\n- On AIME 2024 (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrL03prCCW7Y8-PT6lZ3m-W25XXqs71p54PW2zLmWv2Q6yntW5wL1xV8fTRQGW3kJZyX8x9Xm9VfshpN8hGy12N3kJl_0CQbKTW8TffW74WZkz0W6-tksM5_mV_zN5CCBc-QG1SSN3714xmY07gYW5SRy3d89kQzxW5JvT6415dPT_W5PXldw7Nv1DJW3CZQvR2S1kbKW8MtVBx2W8SKLW1C9mlW6mlGRxW837Zt75ns8pbW80hBqC62SQPXW49sMmg5YnYl4N18jVJKY9KbZW488FsL8f874QW7flFzD82rtS4W7ZHv1m8w9m7LW5gXtQW2vNdpvW196zSN1SxbHJW9lzslS6mcdNtf43__Ys04 ) , competitive high-school math problems, Claude 3.7 Sonnet achieved 80.0 percent in parallel extended thinking mode with a 64,000-token budget. In this test, it underperformed o3-mini with high effort (87.3 percent) and o1 (83.3 percent).\r\n\r\nBehind the news: Anthropic\u2019s approach refines earlier efforts to enable users to control the incremental expense of computing extra tokens at inference. For instance, OpenAI o1 offers three levels of reasoning or \u201ceffort\u201d \u2014 each of which allocates more tokens to reasoning \u2014 while X\u2019s Grok 3 (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrJx5m_5PW50kH_H6lZ3mcW3x3QpX3svp5-W5q7xJy6wd1xyW4gnSWB723pzbW37vPyv8_BhG7W7bg30s7MQv_VW7CQf8T1zMzmyW8-8t-v3qxwQlW228Y9k6wzX1cW7cD3sz1rRd1ZW9jBdkN3484H-W1cf8SH1rD7yRN7gG3S0DZlNlW8kMYnx6Ls297VcPN3Q4_1FgqW886Pf522fmy7N432kw61HQfZW56n6532yYJ1YM-LvXw4kQJqW51dhj952-rTSW12bxXL83BVsYVcD8QY9fKZk1W92qc_N8V8w_7W2DSWM19323H7W7CW1dt7Js_xBW278T9C2rFXLrW4ydJjG5c0kRpW31dsNN8pgW-RW1Jpx2v5q57NFW2BQQd558LcrDW36KWSL2h1zsbN7JgG74LLx32W99XGx42tFCP7f7qz8xM04 )  offers two.\r\n\r\nWhy it matters: Test-time compute (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3nRW47D1g01NHSD7W5sl3qv5v69DhW4DWnxs9h4d5FW8q7m9b5xcyk0W9jrbdd1MZJXLN8tqRN-KhnJlN6l5LMWx1lD6W45S8QP4dSWNsVWT2703rGB_nVzrHDy6nhBzMVpfjHp6nrl82W69NyCM5x30lRN1K7njvzS2zkW35ptlD5yPgJdN6s3jQFdzmxWW3Lh0-g7MYV1gN22nsLkBmVJwVkMq681Fts43W6mRF1l7Jvbw8W2TSqYr19tZBWW6g7ffW8hm5v6W2x4Lmv8qsWZxf1xySMH04 ) , or additional processing at inference, is powerful but expensive, and not all tasks benefit from it. So it\u2019s helpful to let users choose how much to apply. Claude 3.7 Sonnet improves its predecessor\u2019s general performance and provides an ample budget for additional reasoning.\r\n\r\nWe\u2019re thinking: The cost of inference is rising as agentic workflows and other compute-intensive tasks become more widely used. Yet the cost of AI on a per-token basis is falling (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLC3prCCW95jsWP6lZ3n1W89lSrs12JNdFW4_pxpB4d6sKXW3yPQkd6T5YJWW21sgD11pZCLzW7Mbt3b65KLMvW9gRjTC46J5SJW1lbM146Smhc5W60qzjh8v3fnzW7fHZCT1JZ_NYW4Ng07s4CLVYPW3l_8tS8pVv9rN4twd71ct-2pVvymkc1nXQhqW2sjjBq7VdLRrW1GTn5n7tBQ4SW5dtxXy5nz3D3W5sk5Q62p-H85W8xkr7h168kw2N5C3yXQxY9ryW1kTLHh9g914yVc_rv62KX9cpW2bczz934sH8wW5_YdG85-NKcKW6cXKpH4clz7jW832Fr14_RCpxN8SXskx4dnhQVVfVKq2PBPRzW5dsvD27zFf7bW6H6hrT75J_TQW8tWHwP8s_Dvvf6KVgpl04 ) rapidly. Intelligence is becoming steadily cheaper and more plentiful.\r\n\r\nAmazon\u2019s Next-Gen Voice Assistant\r\n\r\nAmazon announced Alexa+, a major upgrade to its long-running voice assistant.\r\n\r\nWhat\u2019s new: Alexa+ (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLj3prCCW8wLKSR6lZ3mxW6k_36y4qkr5PW63Xq345FBH8lW8jHJd338WBj3W1XRPyW3g3K0nV2nTWg5_grCzW59Xc-G6dL62LN9dzWM4w31N_W5CByJP9bSm87VZLfmP4-vy2gW5Lh8Ht5Gw7LNW4f2B0-9bFP_tW2MPtKt7FpKWhW4jt2kf90yXdBW95tw0_4McB8LW436BCN2cGM9PW5Sn7G22F-7KbW6xZW_K1WzJQ1W49zjPm8hrMjgN4xpQYLDbz_KW4Mss-l2rtdR6W3sB7HM4pW0wCMWLvh0mZS9jW81KFd06d9rYmN3fWfxxG9-jCW5NgvTb4TKQBzVMGn4_4bnnv0M-QLkGp9hBKW503c-m4f9xslf6TKx_Y04 ) , which accepts spoken commands and responds conversationally, is designed to work with a variety of vendors as an autonomous agent to make purchases, book reservations, play media, and so on. It will roll out in the U.S. over coming weeks, initially on some Echo Show devices and eventually nearly every current Echo speaker.\r\n\r\nHow it works: Alexa+ updates (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLC3prCCW95jsWP6lZ3mWW5cvTRH3TXN9kW8BSR0f28Fd4rVJ_kBq4j0wtkW6tBFj65B4h4qW59FJfK1KTZkkVRH_7F6vjD0lW7zP01r5P4dTkW6ts8yc2GGHCgW71JyF48VDBCbW1sbhv659-lwnW4rGGcY3rBqLYVHf9yJ1vNjGZN2J2XP9cMZCSW5tpgpq1vR5gsW58BySt24b628VG2nJP5JDBn2W44qm527xrthTW7hRt9k1Mw2VdW1BBsvg4z5Wm-VQRgTX3Gl6ZmW3P4W_f6P3cDBVRTMrW1hH78mVjS7tJ324CWXM_5ZsxD-x8rW11nLL31L0X9RW6brff554kRM2W6NgsRq26K_P_W17MCVt4GCC-zW441m0h2HqmbgN65X3ST8kzZYf24KnYn04 ) the system to take advantage of generative AI including Anthropic Claude, Amazon Nova (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLC3prCCW95jsWP6lZ3n3W6kVb4r61lz2YW38C8Mg4wcVcgW1TRf7f4XLVS2W8Zy38X2h8pYvW7YMNcy6NCYS5Myj1L7tLmvwW1KKr0093D1lNW7SWt_d8QS55yW23TNsN2WjDkfW8sDqmv28y-kkW4zpZWK5wwMqDW1yGwjz5XXqtRW8Rj9hg7h2QdsW8G5qj_7WG3r-W2n485v4k6F7wW4rBVJ55C_gnSW5zs0fs4pkHfrW6W_6Gd7S08tpW3hJF0T7_qQ9MW5nV4TX93sFYmW37hpQB6wLQnVW1Mmw9N7xc6vCW26HHXz4DG02RM3KtcYmZ1FHVMJdXB3T69SJW4x40wg7lN1dmW1K1x5J8nhn9ZW8scCDT6s4sjfV8pvLB3Wyb2FN9kWvg_gGgJ3f8b7-LT04 ) , and other large language models. Inputs are filtered through a routing system that determines the best model to respond to any given request. It\u2019s trained to understand colloquial, conversational language. Its personality is designed to be \u201csmart, considerate, empathetic, and inclusive\u201d as well as humorous.\r\n\r\n- Alexa+  interacts with online vendors to manage smart-home devices (Philips Hue, Ring, Roborock), reserve restaurant seats (OpenTable, Vagaro), play music (Amazon Music, Spotify, Apple Music, iHeartRadio) and videos (Amazon Video, Hulu, Netflix, Disney+), book local service technicians (Thumbtack), and purchase items (Amazon Fresh, Whole Foods, Grubhub, Uber Eats, Ticketmaster). Amazon+ will cost $19.99 per month, free with an Amazon Prime membership ($139 per year). (Disclosure: Andrew Ng is a member of Amazon\u2019s board of directors.)\r\n- The system recognizes individual users and keeps track of personalized information such as dates; recipes, and preferences in sports, food, music, and movies. In addition, it can respond to queries based on purchase records, video and music playbacks, shipping addresses, documents, emails, photos, messages, and so on.\r\n- It can behave proactively, for instance, advising users to start their commute early if traffic is heavy.\r\n- The system calls what Amazon calls experts \u2014 groups of systems, APIs, and instructions \u2014 that orchestrate API calls to accomplish online tasks. For instance, it can navigate and use the web to perform tasks such as finding and booking, say, a local repair service to fix a broken household appliance.\r\n- Alexa+ can deliver timely news and information based on partnerships with news sources including Associated Press, Business Insider, Politico, Reuters, USA Today, and The Washington Post.\r\n\r\nBehind the news: Amazon launched Alexa in 2014, and the voice assistant now resides in over 600 million devices worldwide. However, users relied on it more to set timers, report sports scores, and play music than to purchase products, and Alexa revenue lagged. Following cutbacks in 2021, Amazon made multibillion-dollar (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrJx5m_5PW50kH_H6lZ3pCW45XKhZ984f7yW4sd1yY5SfNV3W8qQ2M08vVJ5MW4phbLg4-5TXyW8h3Sfk2x7hxCW9jl_4d3w4-R-VyT6cL6f6wJ5W2DNgt63_0x88W2QRSvk7HlzS8W3D-hlp58vv9FW4LPKQJ8_Kgl7W5K9fgp2lvjw8N7mj9HdgHmN9Vf0GkW1mf00BW3n5PCs2y99RlVPLhfs76XdchVlpq8p91WqH9W6rgs_p73l23wW5bXHR43z3d2wVjr3l56bH1c0W6JFB-g2yJSPnW6H7nLY7ZRBbfW31_fbK8mdwpSVPRcGB45lGS7Vdwkjd6YWTR-W39y2zg3m4HTtW4KLCYl8f6hVpW70gpPD9kDT09W5n3wL45SRwKDW8wdlvm9l8p9RV_yMlY2FB2Q5W6qdFfm7BRZwmf7wk-3K04 ) investments (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrLC3prCCW95jsWP6lZ3n-W6B8Pbm5-htYgN8Gc1h_grvQYW4zmcPF3VRvbXW1G_Kzx5dxGvbW78wBrv51RhL_W1XGHXW2zcT-sW8Nr6VG8wzyS9W1Jsn4K8bg-v9W92GkCn3ZFy6NVfV4Cm6SmsXFW3nCQcY5HGwjbW1_R4CV219-ZMW65tbWs78txwZW3-wClf9bfkz3N8CXkfrTh7rFW1yvwL75hJjs9W8rlhbn1j11GFW9bpYH988TZTTN7XyMwkwNzH3W5pQXYD201lxHW1ffskq7HtQhbW4K1nPs8N5G7DW8bfzF-85YrKnVx4fXR2KslysW5r9Jlw3WjPf7W4fbBRk4wcjWQN3_cjWCGS9wFW4hdC8f2Q4f1WW5YQ0P48xt3mBVnsxhN8PbF_9f65zhkT04 )  in Anthropic and set about updating the technology for the generative AI era.\r\n\r\nWhy it matters: Alexa, along with Apple\u2019s Siri and Google Assistant, pioneered the market for voice assistants. However, as large language models (LLMs) blossomed, all three systems fell behind the times. (Google allows Android users to substitute one of its Gemini LLMs for Google Assistant, but the system still calls Google Assistant for some tasks.) Alexa+ is the first major voice-assistant update that aims to take advantage of LLMs as well as emerging agentic technology and improved voice interactions, and the rollout is taking these capabilities to a large, existing user base.\r\n\r\nWe\u2019re thinking: Rapid improvements in the voice stack (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKK3prCCW7lCdLW6lZ3psN2-8Dg71BYQmVqLvRz6vMf1PN5dGVTMWS2mfW21q51t5fNp5RW6sl1V58ssyzJW4hRMbM3V34jVW69JVQ597RbClW52z4dX64wqhWW1Z0QdY1hQXJhN5kSscZjGrlRW7xzByr2v0xBdW9cBj2j7VsTczW6npRbn3jWPJsW2D_j6T8DQZYcW2JsLxw7lS_LFVZ2xl42SnMz-W1NXHrv5kZ70XW8hg5hB5_bcx4W2B7G_h51Mh3HW4_RDyn6Q_cNsW4xjJn41NcL-mMBZMXSw38wQW9ccD7K6MXPvsW4Hb2m-638H2Df10-ZNT04 ) are opening doors not only for voice assistants but for a galaxy of applications that rely on spoken input and output. Product designers will need to learn how to design smooth user voice experiences. Watching how Alexa+ manages them will provide useful guidelines.\r\n\r\nWork With Andrew Ng\r\n\r\nJoin the teams that are bringing AI to the world! Check out job openings at DeepLearning.AI (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3pMW3B0qd13ktmHQW5wqTJt8ldkcJW8ZrXKk7hs6hXW6Bp9v-6Lxt49W2N4ywK7gWwLrW33MRRQ5KjzzVW71Q2TR1p0ZRxW2z7ZFj1xwdn1W5Gf1st78BXCSMl8Mnh6xqrBW6lVRN37Ds7RjW6ctLqC70MRg-W2VRFL_77MTkrW4Z4ftR1L2nvrN8j0R7_zwk0kW1mF16b52hC9qW3SpqGj133F4ZW82WDrc8RZ4-sVdVZ9414dVb_W1m9nzk7HGWk2VJ-lsv8NcMz-W3k1PWy1ZTT21f2BzHtP04 ) , AI Fund (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3q5W2xx_G68P3JZ3W2c-XRl3kCvpZW2N7BCl1KDZX2W13kjXB7Lwcm5W12zwm97b658HW15KZ721CwD3vW24K3KP7ZjmMLW8SydHb5-2rbvW5QWC9N65_nJcW6g7-845GQzNNVPyRNf1Ylgn2W6pvDh63hrwmdW34rPPS1GjjXNW525qXc54b1SjW9l3Xrr4XL3CtW28T2Pp1LT-JBV7xsFT1TF0G_VsV_MM3CGgPVVVVp4-2_1BVYN4Xcl_t2BLqFN6RS3yPkF_-JW5G5bHV6fgT5Yf3HLYtP04 ) , and Landing AI (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3prCCW6N1vHY6lZ3l0W8T6Cs428dZ_XW6zQXfr71NCgTW1zsNhb5-fWrtW2NP-GH1_k1lJW6FHFhr2jMkjSW14hX_p9f1W54W6w5SN62T7khxVbqkPs8VDXH2VPBxST1sZ5kBW8n84204YNcGzVCQNw74chnvmVBDjcy1rzcnZW2bg6_m5Lm4_qW6_DyRs1wzhKdW4mC_Ls2CnST9W5xvN1H7hwg8-N7d-3ZT1lVFyW94FlpW67DHtsVwdMLl8f1nPlW2bDqQ61Pt6kdW5ZVH5p8jVrVGW8lJQxC20cnpgf5r7y8404 ) .\r\n\r\nSubscribe and view previous issues here (https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWx_NP1TBhbVW9drZqp5RpmctV9TLwZ5sPXsgN4lQrKq3l5QzW6N1vHY6lZ3mCW6ZVK0x1NKPYHW6xyND06JFZ3LW5x1GCq3kSy-KW81gYxl6Hk3KLW3HVBMN4fpP2YW6VFvt_15N64MW4KXNyt2cj0NXN66vrnHZ68gqN3hJjXrgkWtGW5jdcYM5sLZb8W6xmVQ24vfH-DW7PjHKn5-zLfSW6PbDW35xBSfkN1KK-9-TwV2pW5MbvPD3NFbbFW2KP_dh1GK17LVMJTND6jdlhgW4K2s5J2KGQ60W28Pj3F1nChr3W7Gqg6T95Fd1lW7W3WQ09gZNRZW41v9Jh4_7D-df34Tl1l04 ) .\r\n\r\nThoughts, suggestions, feedback? Please send to thebatch@deeplearning.ai (mailto:thebatch@deeplearning.ai?subject=RE:%20Thoughts,%20Suggestions,%20feedback) . Avoid our newsletter ending up in your spam folder by adding our email address to your contacts list.\r\n\r\nDeepLearning.AI, 195 Page Mill Road, Suite 115, Palo Alto, CA 94306, United States\r\n\r\nUnsubscribe (https://info.deeplearning.ai/hs/preferences-center/en/direct?data=W2nXS-N30h-M3W3FckcK25jFRkW1NdCM-24-nHYW1Ndb7045yGmtW2nSL4G3M2KXXW3Xyyy72KpqqDW2y2_3v2MC2j7W4myxg04psgT8W3d2F4F45pqm3W45Vdnq2nQdBkW2RlM1X2v-644W2z_VzG49n9lmW3Y1NwZ364CD0W36nlxw2KpSv8W3SYLYb2MTY5hW1Bm4Nb2vyjmkW2WGK6g3NFt8kW47vB8H2qC8-9W3Fd1d44tlY-HW4kMYY24fJgKNW4r9XcM2vCBNLW2HLsLD1LGMS_W3K6fxH2MWcn0W2zMY7r32m_McW2KY-sx38hcy7W3gxm1s3_YlbyW2xL1P84hrHN6W36tmy23zcc5cW1Bc-8245BrqcW2Ycrj31_nzg8W3NQGr-384kNfW2WdXLm21lP68W3b492Z2xySdwW1-VwkP45wDQ2W4kn-XC3brL_KW2y0rNp2FFMvKW3BZPxj2MMkCqW3Fbq7Z2YdKtJW1X9lxW3jc2p7W2CGg821BL3DyW2zVdf334nwSsW2zYsvT2WvCwlW45MtV61LB7g9W32wWCS3K8n_vW3j6ssl3jsngjW3dpP103K6fQFf2MzNyv04&_hsenc=p2ANqtz-9SzmqOKLr5DvTOnP6qH0GYQZmz0MEpeHo2cxX57PdV6cRj1jlxTlYbxlax3UUY54G8dUnLcRq6qmE3nNI-kGjhoeoEJsXuepjVc32WiH-ThC3A94E&_hsmi=350282578 )\r\nManage preferences (https://info.deeplearning.ai/hs/preferences-center/en/page?data=W2nXS-N30h-M3W3FckcK25jFRkW1NdCM-24-nHYW1Ndb7045yGmtW2nSL4G3M2KXXW3Xyyy72KpqqDW2y2_3v2MC2j7W4myxg04psgT8W3d2F4F45pqm3W45Vdnq2nQdBkW2RlM1X2v-644W2z_VzG49n9lmW3Y1NwZ364CD0W36nlxw2KpSv8W3SYLYb2MTY5hW1Bm4Nb2vyjmkW2WGK6g3NFt8kW47vB8H2qC8-9W3Fd1d44tlY-HW4kMYY24fJgKNW4r9XcM2vCBNLW2HLsLD1LGMS_W3K6fxH2MWcn0W2zMY7r32m_McW2KY-sx38hcy7W3gxm1s3_YlbyW2xL1P84hrHN6W36tmy23zcc5cW1Bc-8245BrqcW2Ycrj31_nzg8W3NQGr-384kNfW2WdXLm21lP68W3b492Z2xySdwW1-VwkP45wDQ2W4kn-XC3brL_KW2y0rNp2FFMvKW3BZPxj2MMkCqW3Fbq7Z2YdKtJW1X9lxW3jc2p7W2CGg821BL3DyW2zVdf334nwSsW2zYsvT2WvCwlW45MtV61LB7g9W32wWCS3K8n_vW3j6ssl3jsngjW3dpP103K6fQFf2MzNyv04&_hsenc=p2ANqtz-9SzmqOKLr5DvTOnP6qH0GYQZmz0MEpeHo2cxX57PdV6cRj1jlxTlYbxlax3UUY54G8dUnLcRq6qmE3nNI-kGjhoeoEJsXuepjVc32WiH-ThC3A94E&_hsmi=350282578 )",
  "processed": true,
  "processed_at": "2025-03-06T13:38:17.696066"
}